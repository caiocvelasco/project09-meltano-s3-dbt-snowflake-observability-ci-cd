# Defines external sources and their locations within your data warehouse.
# Reference: 
# [1] https://github.com/dbt-labs/dbt-external-tables/blob/main/sample_sources/snowflake.yml
# [2] https://docs.snowflake.com/en/user-guide/tables-external-auto

version: 2

sources:
- name: bronze  # This is the name that will be used as reference in the dbt models. It will be created in Snowflake if it does not exists.     
  description: "External tables using Parquet and inferring the schema"
  database: "{{ env_var('SNOWFLAKE_DATABASE') }}"
  schema: "{{ env_var('SNOWFLAKE_SCHEMA_BRONZE') }}"  # Specifies the external schema where the sources reside
  loader: S3
  tables:
  - name: source_a
    ext_full_refresh: true
    description: "External table - Source A for Customers"
    external:   # Here, we will point to the stage (which is assumed to be created in Snowflake) and we will create the External Tables
      location: "@{{ env_var('SNOWFLAKE_DATABASE') }}.{{ env_var('SNOWFLAKE_SCHEMA_BRONZE') }}.{{ env_var('S3_SNOWFLAKE_STAGE') }}/customers" # reference a FOLDER in S3 where the parquet is located (it will appear in the 'Location' parameter in the 'customers' tables in Snowflake)
      file_format: "{{ env_var('SNOWFLAKE_DATABASE') }}.{{ env_var('SNOWFLAKE_SCHEMA_BRONZE') }}.{{ env_var('S3_SNOWFLAKE_FILE_FORMAT') }}"
      infer_schema: true   # Maps the JSON's 'key:value' pair to the correct columns
      # auto_refresh: true # this will work if an SNS event in S3 happens when a new parquet arrives in S3 # [2]
  - name: source_a